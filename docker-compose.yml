# docker-compose.yml
version: '3.8' # You can remove this line as it's obsolete, it will be ignored, please remove it to avoid potential confusion.

# Define networks for inter-service communication
networks:
  observability_net:
    driver: bridge

# Define all the services
services:
  # 1. Sample API Application
  # This service represents your actual application that will be observed.
  # Now includes OpenTelemetry instrumentation for tracing.
  app:
    build:
      context: ./app
      dockerfile: Dockerfile.app
    ports:
      - "5000:5000" # Expose the application port
    environment:
      - APP_NAME=atlan-api
      - APP_VERSION=1.0.0
      # OpenTelemetry Collector endpoint for traces
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_RESOURCE_ATTRIBUTES=service.name=atlan-api
    networks:
      - observability_net
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    volumes:
      - ./app:/app
    depends_on:
      - otel-collector # App depends on OTel collector for sending traces

  # 2. Prometheus
  # Collects metrics from the application and other services.
  # Now configured with Alertmanager.
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: prometheus
    ports:
      - "9090:9090" # Prometheus UI
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml # Mount Prometheus configuration
      - ./prometheus/alert.rules.yml:/etc/prometheus/alert.rules.yml # Mount alerting rules
      - prometheus_data:/prometheus # Persistent storage for Prometheus data
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--enable-feature=exemplar-storage' # Enable exemplar storage for traces
      - '--web.enable-remote-write-receiver'
      - '--web.enable-lifecycle'
    networks:
      - observability_net
    depends_on:
      - app
      - alertmanager # Prometheus needs Alertmanager to send alerts

  # 3. Grafana
  # Visualizes metrics from Prometheus, logs from Loki, and traces from Jaeger.
  grafana:
    image: grafana/grafana:10.1.5
    container_name: grafana
    ports:
      - "3000:3000" # Grafana UI
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    networks:
      - observability_net
    depends_on:
      - prometheus
      - loki
      - jaeger # Grafana now depends on the unified Jaeger service

  # 4. Loki
  # Log aggregation system, compatible with Grafana.
  loki:
    image: grafana/loki:2.9.2
    container_name: loki
    ports:
      - "3100:3100" # Loki's ingestion port
    volumes:
      - loki_data:/loki
      - ./loki/loki-config.yaml:/etc/loki/config.yaml
    command: -config.file=/etc/loki/config.yaml
    networks:
      - observability_net

  # 5. Promtail
  # Agent to ship logs from Docker containers to Loki.
  promtail:
    image: grafana/promtail:2.9.2
    container_name: promtail
    volumes:
      - ./promtail/promtail-config.yaml:/etc/promtail/config.yaml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yaml
    networks:
      - observability_net
    depends_on:
      - loki

  # 6. OpenTelemetry Collector (for Tracing)
  # Receives traces from the app and forwards them to Jaeger.
  otel-collector:
    image: otel/opentelemetry-collector:0.87.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    # The 'ports' section has been removed entirely here.
    networks:
      - observability_net
    depends_on:
      - jaeger # OTel collector now depends on the unified Jaeger service

  # 7. Unified Jaeger (all-in-one)
  # Combines Jaeger Collector, Query, and Agent, with in-memory storage.
  jaeger:
    image: jaegertracing/all-in-one:1.50 # Using the all-in-one image
    container_name: jaeger
    environment:
      # Explicitly set to in-memory storage. This is crucial for the all-in-one image.
      - MEMORY_MAX_TRACES=50000
      - COLLECTOR_OTLP_GRPC_PORT=4317
      - COLLECTOR_OTLP_HTTP_PORT=4318
    ports:
      - "16686:16686" # Jaeger UI
      - "4317:4317" # OTLP gRPC endpoint for collector (Host:Container)
      - "4318:4318" # OTLP HTTP endpoint for collector (Host:Container)
      - "14268:14268" # Jaeger HTTP Thrift for collector
      - "14250:14250" # Jaeger gRPC for collector
    networks:
      - observability_net

  # 8. Prometheus Alertmanager
  # Handles alerts sent by Prometheus.
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager
    ports:
      - "9093:9093" # Alertmanager UI
    volumes:
      - ./alertmanager/config.yml:/etc/alertmanager/config.yml # Alertmanager configuration
      - alertmanager_data:/alertmanager # ADDED: Persistent storage for Alertmanager data
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--web.external-url=http://localhost:9093'
    networks:
      - observability_net

  # 9. Locust (Load Generator)
  # Continuously generates load on the API endpoint.
  locust:
    build:
      context: ./locust # Build from a dedicated Locust directory
      dockerfile: Dockerfile.locust
    container_name: locust
    ports:
      - "8089:8089" # Locust UI
    environment:
      - HOST=http://app:5000 # Target host is our app service
    networks:
      - observability_net
    depends_on:
      - app # Locust needs the app to be running

# Define persistent volumes for data
volumes:
  prometheus_data:
  grafana_data:
  loki_data:
  alertmanager_data:
